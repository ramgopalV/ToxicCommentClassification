{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToxicCommentClassification_LSTM_Kaggle.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "C-29buciiOlI",
        "outputId": "c20141d0-cb37-40e8-a080-8975359aea2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "M5yFMVgOvqri",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "f6VBnqNSjv-D",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 100\n",
        "# train = pd.read_csv(io.StringIO(uploaded['train.csv'].decode('utf-8')))\n",
        "# test = pd.read_csv(io.StringIO(uploaded['test.csv'].decode('utf-8')))\n",
        "\n",
        "train=pd.read_csv('drive/My Drive/Toxic_comment/train.csv') \n",
        "test=pd.read_csv('drive/My Drive/Toxic_comment/test.csv') \n",
        "embedding_file = 'drive/My Drive/Toxic_comment/glove.6B.50d.txt'\n",
        "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "y = train[list_classes].values\n",
        "train.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K_X01d0QqMbJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = train['question_text']\n",
        "y_train = train['target']\n",
        "x_test = test['question_text']\n",
        "print(train.apply(lambda x: sum(x.isnull())))\n",
        "print(test.apply(lambda x: sum(x.isnull())))\n",
        "maxlen = len(max(x_train))\n",
        "print(len(x_train))\n",
        "print(len(y_train))\n",
        "print(y_train.unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oeb8ngZIrVoZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_embedding_coefs(word, *arr): \n",
        "  return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_embedding_coefs(*o.strip().split()) for o in open(embedding_file,'r', encoding=\"utf8\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n6_ZMCYCQxH5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(list_sentences_train))\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
        "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dU3uw0RjQxIG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = 50\n",
        "i\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HkEhSlZDrKIm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    embed_size = 128\n",
        "    inp = Input(shape=(maxlen, ))\n",
        "    x = Embedding(max_features, embed_size)(inp)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True,dropout=0.5,recurrent_dropout=0.1))(x)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(50, activation=\"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(6, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "USeocgYdQxIe",
        "colab_type": "code",
        "outputId": "37ea9e56-df30-40d6-fcd2-9275f399003e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "model.fit(X_train, y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 104589 samples, validate on 11622 samples\n",
            "Epoch 1/10\n",
            "104589/104589 [==============================] - 1886s 18ms/step - loss: 0.0874 - acc: 0.9728 - val_loss: 0.0495 - val_acc: 0.9824\n",
            "Epoch 2/10\n",
            "104589/104589 [==============================] - 1878s 18ms/step - loss: 0.0532 - acc: 0.9812 - val_loss: 0.0489 - val_acc: 0.9825\n",
            "Epoch 3/10\n",
            "104589/104589 [==============================] - 1853s 18ms/step - loss: 0.0482 - acc: 0.9823 - val_loss: 0.0487 - val_acc: 0.9829\n",
            "Epoch 4/10\n",
            "104589/104589 [==============================] - 1855s 18ms/step - loss: 0.0445 - acc: 0.9834 - val_loss: 0.0516 - val_acc: 0.9825\n",
            "Epoch 5/10\n",
            "104589/104589 [==============================] - 1835s 18ms/step - loss: 0.0414 - acc: 0.9842 - val_loss: 0.0536 - val_acc: 0.9823\n",
            "Epoch 6/10\n",
            "104589/104589 [==============================] - 1877s 18ms/step - loss: 0.0390 - acc: 0.9850 - val_loss: 0.0601 - val_acc: 0.9817\n",
            "Epoch 7/10\n",
            "104589/104589 [==============================] - 1852s 18ms/step - loss: 0.0368 - acc: 0.9856 - val_loss: 0.0580 - val_acc: 0.9818\n",
            "Epoch 8/10\n",
            "  1152/104589 [..............................] - ETA: 29:18 - loss: 0.0324 - acc: 0.9848"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9rFfuFXUQxI8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test = model.predict(X_test)\n",
        "sample_submission = pd.read_csv('drive/My Drive/Toxic_comment/sample_submission.csv')\n",
        "sample_submission[list_classes] = y_test\n",
        "sample_submission.to_csv(\"baseline.csv\", index=False)\n",
        "files.download('baseline.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}